import asyncio
import os
import sqlite3
import logging
from typing import Optional, Tuple, List

import pandas as pd
from aiogram import Bot, Dispatcher, F, Router
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.filters import CommandStart, Command
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.types import Message, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery
from dotenv import load_dotenv
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# OpenAI
from openai import OpenAI

# ---------------- Logging ----------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s")
log = logging.getLogger("boot")

# ---------------- Env ----------------
load_dotenv()
BOT_TOKEN = (os.getenv("BOT_TOKEN") or "").strip()
SUPPORT_CHAT_ID = int((os.getenv("SUPPORT_CHAT_ID") or "0").strip() or "0")
SIMILARITY_THRESHOLD = float((os.getenv("SIMILARITY_THRESHOLD") or "0.4").strip() or "0.4")
USE_LLM_FALLBACK = (os.getenv("USE_LLM_FALLBACK","true").lower() in ["1","true","yes"])
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()
OPENAI_MODEL = (os.getenv("OPENAI_MODEL") or "gpt-4o-mini").strip()

if not BOT_TOKEN:
    raise SystemExit("Please set BOT_TOKEN in .env")

# ---------------- KB loader ----------------
DATA_PATH = os.path.join(os.path.dirname(__file__), "data", "knowledge_base.csv")

kb_df: pd.DataFrame
vectorizer: TfidfVectorizer
kb_matrix = None

def load_kb():
    global kb_df, vectorizer, kb_matrix
    if not os.path.exists(DATA_PATH):
        raise SystemExit(f"Knowledge base not found at {DATA_PATH}")
    kb_df = pd.read_csv(DATA_PATH)
    if not set(["question", "answer", "url"]).issubset(kb_df.columns):
        raise SystemExit("CSV must contain columns: question, answer, url")
    vectorizer = TfidfVectorizer(analyzer="char", ngram_range=(3, 5))
    kb_matrix = vectorizer.fit_transform(kb_df["question"].fillna("").astype(str))
    log.info(f"KB loaded: {len(kb_df)} rows")

def top_k_context(user_text: str, k: int = 5) -> List[dict]:
    q = vectorizer.transform([user_text])
    sims = cosine_similarity(q, kb_matrix)[0]
    idxs = sims.argsort()[::-1][:k]
    rows = []
    for i in idxs:
        row = kb_df.iloc[int(i)]
        rows.append({"question": str(row["question"]), "answer": str(row["answer"]), "url": str(row["url"])})
    return rows

def answer_from_kb(user_text: str) -> Tuple[Optional[str], float]:
    if not user_text or not user_text.strip():
        return None, 0.0
    q = vectorizer.transform([user_text])
    sims = cosine_similarity(q, kb_matrix)[0]
    best_idx = sims.argmax()
    best_score = float(sims[best_idx])
    if best_score >= SIMILARITY_THRESHOLD:
        row = kb_df.iloc[best_idx]
        url = str(row.get("url") or "").strip()
        ans = str(row.get("answer") or "").strip()
        if url:
            ans = f"{ans}\n\n–ü–æ–¥—Ä–æ–±–Ω–µ–µ: {url}"
        return ans, best_score
    return None, best_score

load_kb()

# ---------------- OpenAI LLM fallback ----------------
client = None
if USE_LLM_FALLBACK and OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)
    log.info(f"LLM fallback enabled with model={OPENAI_MODEL}")
else:
    if USE_LLM_FALLBACK:
        log.warning("LLM fallback requested but OPENAI_API_KEY not set. Fallback disabled.")
    USE_LLM_FALLBACK = False

def llm_answer(user_text: str) -> Optional[str]:
    if not client:
        log.warning("LLM client is not initialized")
        return None
    ctx_items = top_k_context(user_text, k=5)
    ctx_str = "\n\n".join([f"Q: {it['question']}\nA: {it['answer']}" for it in ctx_items])
    sys_prompt = (
        "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å–∞–π—Ç–∞ https://meff.netlify.app/. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É. "
        "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (FAQ/—É—Å–ª—É–≥–∏ –∫–æ–º–ø–∞–Ω–∏–∏/–≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å–∞–π—Ç—É). "
        "–ï—Å–ª–∏ —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏, —á—Ç–æ –Ω–µ —É–≤–µ—Ä–µ–Ω, –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º."
    )
    user_prompt = (
        f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_text}\n\n"
        f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{ctx_str}\n\n"
        "–ï—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ, –¥–æ–±–∞–≤—å –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫—É: '–ü–æ–¥—Ä–æ–±–Ω–µ–µ: https://meff.netlify.app/'"
    )
    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        # –ü–æ–∫–∞–∂–µ–º –≤ –ª–æ–≥–∞—Ö –ø–æ–ª–Ω—É—é –æ—à–∏–±–∫—É ‚Äî —Å—Ä–∞–∑—É –≤–∏–¥–Ω–æ 401/–ø—Ä–æ–µ–∫—Ç/–ª–∏–º–∏—Ç
        import traceback
        log.error(f"LLM error: {e}\n{traceback.format_exc()}")
        return None

# ---------------- DB (tickets) ----------------
DB_PATH = os.path.join(os.path.dirname(__file__), "data", "bot.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
conn = sqlite3.connect(DB_PATH)
cur = conn.cursor()
cur.execute("""CREATE TABLE IF NOT EXISTS tickets (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    support_msg_id INTEGER,
    status TEXT DEFAULT 'open'
)""")
conn.commit()

def create_ticket(user_id: int, support_msg_id: Optional[int]) -> int:
    cur.execute("INSERT INTO tickets (user_id, support_msg_id, status) VALUES (?, ?, 'open')",
                (user_id, support_msg_id))
    conn.commit()
    return cur.lastrowid

def get_user_id_by_support_msg(support_msg_id: int) -> Optional[int]:
    cur.execute("SELECT user_id FROM tickets WHERE support_msg_id = ?", (support_msg_id,))
    row = cur.fetchone()
    return int(row[0]) if row else None

# ---------------- FSM ----------------
class SupportForm(StatesGroup):
    waiting_for_description = State()

# ---------------- Bot & Router ----------------
bot = Bot(BOT_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
dp = Dispatcher()
router = Router()
dp.include_router(router)

# ---------------- Keyboards ----------------
def main_menu_kb():
    return InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å", callback_data="ask")],
        [InlineKeyboardButton(text="–ù–∞–ø–∏—Å–∞—Ç—å –æ–ø–µ—Ä–∞—Ç–æ—Ä—É", callback_data="support")]
    ])

def escalate_kb():
    return InlineKeyboardMarkup(inline_keyboard=[
        [
            InlineKeyboardButton(text="–û—Ç–ø—Ä–∞–≤–∏—Ç—å –æ–ø–µ—Ä–∞—Ç–æ—Ä—É", callback_data="esc_yes"),
            InlineKeyboardButton(text="–ù–µ –Ω–∞–¥–æ", callback_data="esc_no")
        ]
    ])

# ---------------- Commands ----------------
@router.message(CommandStart())
async def on_start(m: Message):
    await m.answer(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ç–≤–µ—á–∞—é –Ω–∞ —á–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã 24/7 "
        "–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥–∞–º –∑–∞–ø—Ä–æ—Å –∂–∏–≤–æ–º—É –æ–ø–µ—Ä–∞—Ç–æ—Ä—É.",
        reply_markup=main_menu_kb()
    )

@router.message(Command("help"))
async def on_help(m: Message):
    await m.answer(
        "–û—Ç–ø—Ä–∞–≤—å—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º ‚Äî —è –ø—Ä–æ–≤–µ—Ä—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.\n"
        "–ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–∞–ª–æ, –ø—Ä–µ–¥–ª–æ–∂—É —ç—Å–∫–∞–ª–∞—Ü–∏—é –æ–ø–µ—Ä–∞—Ç–æ—Ä—É.\n\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "‚Ä¢ /start ‚Äî –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é\n"
        "‚Ä¢ /support ‚Äî –Ω–∞–ø–∏—Å–∞—Ç—å –æ–ø–µ—Ä–∞—Ç–æ—Ä—É\n"
        "‚Ä¢ /id ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å chat_id (–¥–ª—è –≥—Ä—É–ø–ø—ã)\n"
        "‚Ä¢ /reload ‚Äî –ø–µ—Ä–µ—á–∏—Ç–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π\n"
        "‚Ä¢ /mode ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∂–∏–º—ã –æ—Ç–≤–µ—Ç–∞"
    )

@router.message(Command("reload"))
async def cmd_reload(m: Message):
    try:
        load_kb()
        await m.answer(f"KB reloaded: {len(kb_df)} entries.")
    except Exception as e:
        await m.answer(f"Reload error: {e}")

@router.message(Command("llm"))
async def cmd_llm(m: Message):
    if not USE_LLM_FALLBACK:
        await m.answer("LLM fallback –≤—ã–∫–ª—é—á–µ–Ω (USE_LLM_FALLBACK=false).")
        return
    if not OPENAI_API_KEY:
        await m.answer("OPENAI_API_KEY –ø—É—Å—Ç ‚Äî –¥–æ–±–∞–≤—å –∫–ª—é—á –≤ .env.")
        return
    q = (m.text or "").split(maxsplit=1)
    if len(q) < 2:
        await m.answer("–ù–∞–ø–∏—à–∏: <code>/llm —Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å</code>")
        return
    test_q = q[1]
    ans = llm_answer(test_q)
    if ans:
        await m.answer("–û—Ç–≤–µ—Ç LLM:\n\n" + ans)
    else:
        await m.answer("LLM –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª. –°–º–æ—Ç—Ä–∏ –ª–æ–≥–∏ –∫–æ–Ω—Å–æ–ª–∏ ‚Äî —Ç–∞–º –±—É–¥–µ—Ç –ø—Ä–∏—á–∏–Ω–∞ (401/429/–ø—Ä–æ–µ–∫—Ç –∏ —Ç.–ø.).")

@router.message(Command("mode"))
async def cmd_mode(m: Message):
    await m.answer(
        f"Similarity threshold: {SIMILARITY_THRESHOLD}\n"
        f"LLM fallback: {'ON' if USE_LLM_FALLBACK else 'OFF'} ({OPENAI_MODEL if USE_LLM_FALLBACK else ''})"
    )

@router.message(Command("support"))
async def support_cmd(m: Message, state: FSMContext):
    await state.set_state(SupportForm.waiting_for_description)
    await m.answer("–û–ø–∏—à–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–∞—à—É –ø—Ä–æ–±–ª–µ–º—É –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º ‚Äî —è –ø–µ—Ä–µ–¥–∞–º –æ–ø–µ—Ä–∞—Ç–æ—Ä—É.")

@router.message(Command("id"))
async def cmd_id(m: Message):
    await m.answer(f"chat_id = <code>{m.chat.id}</code>\nchat_type = {m.chat.type}")
    log.info(f"/id -> chat_id={m.chat.id}, type={m.chat.type}, title={getattr(m.chat, 'title', '')}")

# ---------------- Callbacks ----------------
@router.callback_query(F.data == "ask")
async def ask_cb(cq: CallbackQuery):
    await cq.message.answer("–ù–∞–ø–∏—à–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º. –Ø –ø—Ä–æ–≤–µ—Ä—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.")
    await cq.answer()

@router.callback_query(F.data == "support")
async def support_cb(cq: CallbackQuery, state: FSMContext):
    await state.set_state(SupportForm.waiting_for_description)
    await cq.message.answer("–û–ø–∏—à–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–∞—à—É –ø—Ä–æ–±–ª–µ–º—É –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º ‚Äî —è –ø–µ—Ä–µ–¥–∞–º –æ–ø–µ—Ä–∞—Ç–æ—Ä—É.")
    await cq.answer()

@router.callback_query(F.data == "esc_yes")
async def esc_yes_cb(cq: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    original_q = data.get("last_question")
    await state.set_state(SupportForm.waiting_for_description)
    if original_q:
        await state.update_data(original_question=original_q)
    await cq.message.answer("–û–∫–µ–π, –Ω–∞–ø–∏—à–∏—Ç–µ –∫—Ä–∞—Ç–∫–æ –¥–µ—Ç–∞–ª–∏ (1 —Å–æ–æ–±—â–µ–Ω–∏–µ–º) ‚Äî –ø–µ—Ä–µ–¥–∞–º –æ–ø–µ—Ä–∞—Ç–æ—Ä—É –≤–º–µ—Å—Ç–µ —Å –≤–∞—à–∏–º –≤–æ–ø—Ä–æ—Å–æ–º.")
    await cq.answer()

@router.callback_query(F.data == "esc_no")
async def esc_no_cb(cq: CallbackQuery):
    await cq.message.answer("–•–æ—Ä–æ—à–æ! –ï—Å–ª–∏ —á—Ç–æ, –≤—Å–µ–≥–¥–∞ –º–æ–∂–Ω–æ –Ω–∞–±—Ä–∞—Ç—å /support.")
    await cq.answer()

# ---------------- User text ----------------
@router.message(SupportForm.waiting_for_description)
async def handle_support_description(m: Message, state: FSMContext):
    details = m.text or ""
    data = await state.get_data()
    original_q = data.get("original_question")
    user = m.from_user
    title = f"–ù–æ–≤—ã–π —Ç–∏–∫–µ—Ç –æ—Ç @{user.username or '–±–µ–∑_—é–∑–µ—Ä–Ω–µ–π–º–∞'} (ID {user.id})"
    block = f"<b>{title}</b>\n"
    if original_q:
        block += f"\n–ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å:\n<blockquote>{original_q}</blockquote>\n"
    block += f"–û–ø–∏—Å–∞–Ω–∏–µ:\n<blockquote>{details}</blockquote>"
    if SUPPORT_CHAT_ID == 0:
        await m.answer("–û—à–∏–±–∫–∞: –Ω–µ –∑–∞–¥–∞–Ω SUPPORT_CHAT_ID –≤ .env")
        await state.clear()
        return
    sent = await bot.send_message(chat_id=SUPPORT_CHAT_ID, text=block)
    t_id = create_ticket(user_id=user.id, support_msg_id=sent.message_id)
    await m.answer(f"–°–ø–∞—Å–∏–±–æ! –°–æ–∑–¥–∞–Ω —Ç–∏–∫–µ—Ç ‚Ññ{t_id}. –û–ø–µ—Ä–∞—Ç–æ—Ä —Å–∫–æ—Ä–æ –æ—Ç–≤–µ—Ç–∏—Ç –∑–¥–µ—Å—å.")
    await state.clear()

@router.message(F.chat.type == "private")
async def handle_user_question(m: Message, state: FSMContext):
    q = (m.text or "").strip()
    if not q:
        return

    # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    await state.update_data(last_question=q)

    # 2. –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    ans, score = answer_from_kb(q)
    if ans and score >= SIMILARITY_THRESHOLD:
        await m.answer(ans)
        return

    # 3. –ï—Å–ª–∏ –±–∞–∑–∞ –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ ‚Äî —Å–ø—Ä–∞—à–∏–≤–∞–µ–º LLM
    if USE_LLM_FALLBACK:
        gen = llm_answer(q)
        if gen:
            await m.answer(gen)
            return

    # 4. –ï—Å–ª–∏ –∏ LLM –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª ‚Äî —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä
    await m.answer(
        "–Ø –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ ü§î. –•–æ—á–µ—à—å, –ø–µ—Ä–µ–¥–∞–º –≤–æ–ø—Ä–æ—Å –æ–ø–µ—Ä–∞—Ç–æ—Ä—É?",
        reply_markup=escalate_kb()
    )

# ---------------- Support group relay + debug ----------------
@router.message()
async def support_group_relay(m: Message):
    if SUPPORT_CHAT_ID and (m.chat and m.chat.id == SUPPORT_CHAT_ID) and m.reply_to_message:
        ref_id = m.reply_to_message.message_id
        user_id = get_user_id_by_support_msg(ref_id)
        if user_id:
            text = m.text or "(–±–µ–∑ —Ç–µ–∫—Å—Ç–∞)"
            try:
                await bot.send_message(chat_id=user_id, text=f"–û—Ç–≤–µ—Ç –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞:\n\n{text}")
            except Exception:
                pass
    if m.chat and m.chat.type != "private":
        logging.getLogger("group").info(
            f"[DEBUG] chat_id={m.chat.id}, type={m.chat.type}, title={getattr(m.chat, 'title', '')}"
        )

# ---------------- Run ----------------
async def main():
    log.info("Bot is starting...")
    if SUPPORT_CHAT_ID == 0:
        log.warning("WARNING: SUPPORT_CHAT_ID is 0. Set it in .env after /id in the group.")
    me = await bot.get_me()
    log.info(f"getMe OK: @{me.username} (id={me.id})")
    await dp.start_polling(bot)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        log.exception(f"FATAL: {e}")
        raise
